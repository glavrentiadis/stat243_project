---
title: "Final Project 2018 - STAT243"
author: "Bunker, JD; Bui, Anh; Lavrentiadis, Grigorios"
date: "December 10, 2018"
output: pdf_document
---

#I. Purpose 
This project aims to apply the adaptive rejection sampling method described in Gilks et al. (1992). The main function \textit{ars()} generates a sample from any univariate log-concave probability density function. In nonadaptive rejection sampling, the envelope and squeezing functions, $g_l(x)$ and $g_u(x)$, are determined in advance. Under the adaptive rejection sampling framework, these functions are instead updated iteratively as points are sampled.

#II. Contributions of members

Each team member was primarily responsible for one of the three steps mentioned in Gilks et al. (1992): (1) the initialization step, (2) the sampling step, (3) the updating step. Summary of contributions:
\begin{itemize}

\item Bunker, JD: Primarily responsible for the initialization step where the . If the user does not provide pointswhich samples the starting points. The \textit{initial()} function . Other contributions include improving the computational efficiency of the \textit{Update_accept()} function, and testing the log-concavity of the given function.

\item Lavrentiadis, Grigorios: Primarily responsible for the sampling step, which generates the value $x^*$ from a piecewise exponential distribution, $s_k(x)$. Assemble the auxiliary functions into the comprehensive \textit{ars()} function.

\item Bui, Anh: Update_accept function to decide whether $x^*$ is accepted or rejected in the final sample.  Update the envelop and squeezing functions accordingly.  Write the report and the formal testing.

\end{itemize}

#III.  Theoretical background for rejection sampling

Assume f(x) is an univariate log-concave probability density function.  We sample from g(x), which is a scaling of function f(x).  Assuming that we have the envelop function $g_{u}(x)$ and the squeezing function $g_{l}(x)$ of g(x).

Let Y ~ Unif(0,$g_{u}(x)$) and where w ~ Unif(0,1).  We reject $x*$ if 

$$
Y < g(x*) 
$$
$$
\frac{Y}{g_{u}(x)} < \frac{g(x)}{g_{u}(x)}
$$
$$
w < \frac{g(x)}{g_{u}(x)} 
$$

The paper gives the algorithm of adaptive rejection sampling when working with the log of g(x), $g_{l}(x)$, and $g_{u}$, which will be discussed more in details in the auxiliary functions section.

#IV. Auxiliary functions

##1. Initial function

##2. Generate x* from piecewise exponential probabilities

The `SamplePieceExp` function draws samples $x^*$ out of a piece-wise exponential distribution using the inverse sampling approach. 
Initially a $Pinv$ sample is drawn from a $0$ to $1$ uniform distribution that corresponds to the cumulative probability of the random sample $x^*$
To find the bin at which $x^*$ belongs, $Pinv$ is compared with the cumulative probability of each bin. 
$x^*$ belongs to the bin whose cumulative probability ($Pcum_i$) is the smallest out of all bins that have $Pcum$ larger than $Pinv$.
$x^*$ is estimated by solving the following cumulative probability equation, where $z_0$ is the left bound of the distribution and $P_j$ is the probability of bin $j$.


$$
P_{inv} = \int^{x^*}_{z_0} s(x) dx = \sum^i_{j=1}P_j + \int^{x^*}_{z_i} s(x) dx
$$

$DP$ equals to the probability $P(z_i > x > x^*)$ where $z_i$ is the lower bound of the bin $i$ where $x^*$ belongs

$$
DP = \int^{x^*}_{z_i} e^{h(x_j) + (x-x_j) h'(x_j)} dx 
$$

To simplify the equation we define: $a = h(x_j)-x_j h'(x_j)$ and $b = h'(x_j)$
From this equation, $x^*$ equals to:

$$
x^* = \frac{1}{b} log(DP~b~e^{-a} +  e^{b~z_i})
$$

##3. Update accept function

**Algorithm**

Inputs: w ~ Unif(0,1) \newline
$$l_{k}(x^{*}) = log(g_{l}(x^{*}))$$ 
$$u_{k}(x^{*}) = log(g_{u}(x^{*}))$$ 
$$h(x^{*}) = log(g(x^{*}))$$
$$s_{k}(x) = exp(u_{k}(x))/\left(\int_{D} u_{k}(x') \; dx'\right) = g_{u}(x)/\left(\int_{D} g_{u}(x') \; dx'\right)$$ 

The lower bound of h(x) is $l_{k}(x)$, which connects the values of function h on abscissaes.
The function of $l_{k}(x)$ between two consecutive abscissaes $x_{j}$ and $x_{j+1}$ is
  $$l_{k}(x) = \frac{(x_{j+1} - x)h(x_{j}) + (x - x_{j})h(x_{j+1})}{x_{j+1} - x_{j}}$$ 
Let X be the domain of abscissaes, H be the domain of the realized function H at abscissaes, H_prime be the domain of the realized first derivative of function H at abscissaes, Z be the domain of intersection of tangent lines at abscissaes.

$$h'(x) = \frac{dlog(g(x))}{dx} = \frac{g'(x)}{g(x)}$$
The intersection of the tangents at $x_{j}$ and $x_{j+1}$ is

$$z_j = \frac{h(x_{j+1})-h(x_{j}) - x_{j+1}h'(x_{j+1})+x_{j}h'(x_{j})}{h'(x_{j})-h'(x_{j+1})}$$
Then for x between $z_{j-1}$ and $z_{j}$
$$u_{k}(x) = h(x_{j}) + (x-x_{j})h'(x_{j})$$

**Step 1**:
If $w < exp(l_{k}(x^{*}) - u_{k}(x^{*}))$ \newline
- Accept $x^{*}$ when the condition is satisfied. Draw another $x^{*}$ from $s_{k}(x)$ \newline
- Reject $x^{*}$ when the condition is not satisfied.

**Step 2**: These two procedures can be done in parallel. \newline
- Evaluate $h(x^{*}), h'(x^{*})$. Update $l_{k}(x), u_{k}(x), s_{k}(x)$. X includes $x^{*}$ as an element. \newline
- Accept $x^{*}$ if $w < exp(h(x^{*}) - u_{k}(x^{*}))$. Otherwise, reject.

Since the h(x), $l_{k}(x)$, $u_{k}(x)$ can be generated from vectors H, H_prime, and Z, we improve the efficiency of the calculation by efficiently updating H, H_prime, and Z.  We append the vectors associated with the new abscissae x* and append to the existing vectors.

Multiple testing are generated based on values that $x^*$ can take.  For example, if $x^*$ is out of the domain of X, $l_{k}(x^{*})=-Inf$. If $x^*$ is at the minimum value X[1] and maximum value X[n] in the domain of X, $l_{k}(x^{*})$ will take the values on the lines connecting X[1] and X[2], and connecting X[n-1] and X[n] respectively.  Vector Z is also generated for the case when H(x) is a linear function of x (for example, the exponential distribution)

#V. Testing

##1. Formal tests for ars function
The ars function passes the test for the following distributions in the testing phase using the Kolmogorov-Smirnov Test: \newline
- Normal distribution with mean = 0  and standard deviation = 1 \newline
- Normal distribution with mean = 7  and standard deviation = 2 \newline
- Beta(1,3) distribution \newline
- Gamma(2,3) distribution \newline
- Exponential(5) distribution \newline

##2. Tests for auxiliary functions

###a. Test CalcProbBin function
The CalcProbBin function generates the cumulative probability based on elements in vector Z (i.e. the intersection of the tangent lines of abscissaes).Based on vector Z under the N(0,1) density, we test whether the calculated cumulative probabilities are (nearly) equal to the cumulative probability using "pnorm(Z)".

###b. Test UpdateAccept function
The UpdateAccept function decides whether to accept $x*$ or not based on designed conditions.  We test this function by checking that if $x*$ is included in X, $x*$ is accepted and included in the x_accept vector, as well as there is no update, i.e. H and H_prime stay the same.
